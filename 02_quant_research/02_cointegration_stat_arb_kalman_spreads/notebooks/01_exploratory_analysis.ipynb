{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## Statistical Arbitrage Research Project\n",
    "\n",
    "This notebook performs initial exploratory analysis including:\n",
    "- Data loading and quality checks\n",
    "- Price series visualization\n",
    "- Return distribution analysis\n",
    "- Correlation analysis\n",
    "- Stationarity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_pipeline import DataPipeline\n",
    "from stationarity_tests import StationarityTester\n",
    "from utils import load_config\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/strategy_config.yaml')\n",
    "\n",
    "# Initialize data pipeline\n",
    "pipeline = DataPipeline()\n",
    "\n",
    "# Download data\n",
    "tickers = config['data']['universe']\n",
    "start_date = config['data']['start_date']\n",
    "end_date = config['data']['end_date']\n",
    "\n",
    "print(f\"Downloading data for {len(tickers)} tickers...\")\n",
    "prices = pipeline.download_data(tickers, start_date, end_date)\n",
    "\n",
    "print(f\"\\nData shape: {prices.shape}\")\n",
    "print(f\"Date range: {prices.index[0]} to {prices.index[-1]}\")\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "missing_data = prices.isnull().sum()\n",
    "print(\"Missing data by ticker:\")\n",
    "print(missing_data)\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(prices.isnull(), cbar=False, yticklabels=False)\n",
    "plt.title('Missing Data Pattern')\n",
    "plt.xlabel('Ticker')\n",
    "plt.ylabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/missing_data_heatmap.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "processed_prices = pipeline.preprocess()\n",
    "\n",
    "# Get summary statistics\n",
    "summary_stats = pipeline.get_summary_statistics()\n",
    "print(\"\\nSummary Statistics:\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Price Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized prices\n",
    "normalized_prices = processed_prices / processed_prices.iloc[0] * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "for col in normalized_prices.columns:\n",
    "    ax.plot(normalized_prices.index, normalized_prices[col], label=col, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Normalized Price (Base=100)', fontsize=12)\n",
    "ax.set_title('Normalized Price Series', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/normalized_prices.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Return Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute returns\n",
    "returns = processed_prices.pct_change().dropna()\n",
    "\n",
    "# Plot return distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(returns.columns):\n",
    "    axes[i].hist(returns[col], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{col} Returns', fontsize=10)\n",
    "    axes[i].set_xlabel('Return', fontsize=9)\n",
    "    axes[i].set_ylabel('Frequency', fontsize=9)\n",
    "    axes[i].axvline(0, color='red', linestyle='--', linewidth=1)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/return_distributions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return statistics\n",
    "return_stats = pd.DataFrame({\n",
    "    'Mean': returns.mean() * 252,\n",
    "    'Volatility': returns.std() * np.sqrt(252),\n",
    "    'Sharpe': (returns.mean() / returns.std()) * np.sqrt(252),\n",
    "    'Skewness': returns.skew(),\n",
    "    'Kurtosis': returns.kurtosis()\n",
    "})\n",
    "\n",
    "print(\"\\nAnnualized Return Statistics:\")\n",
    "return_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = processed_prices.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Price Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/correlation_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated pairs\n",
    "print(\"\\nHighly Correlated Pairs (>0.8):\")\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.8:\n",
    "            print(f\"{corr_matrix.columns[i]} - {corr_matrix.columns[j]}: {corr_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stationarity tester\n",
    "tester = StationarityTester(significance_level=0.05)\n",
    "\n",
    "# Test all price series\n",
    "print(\"Testing stationarity of price series...\\n\")\n",
    "price_stationarity = tester.test_multiple_series(processed_prices, test_type='combined')\n",
    "price_stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test return series (should be stationary)\n",
    "print(\"\\nTesting stationarity of return series...\\n\")\n",
    "return_stationarity = tester.test_multiple_series(returns, test_type='combined')\n",
    "return_stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_ratio = config['data']['train_test_split']\n",
    "train_prices, test_prices = pipeline.train_test_split(train_ratio=train_ratio)\n",
    "\n",
    "print(f\"Training set: {len(train_prices)} observations\")\n",
    "print(f\"Testing set: {len(test_prices)} observations\")\n",
    "\n",
    "# Visualize split\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(train_prices.index, train_prices['SPY'], label='Train', color='blue', linewidth=2)\n",
    "ax.plot(test_prices.index, test_prices['SPY'], label='Test', color='red', linewidth=2)\n",
    "ax.axvline(train_prices.index[-1], color='black', linestyle='--', linewidth=2, label='Split')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('SPY Price', fontsize=12)\n",
    "ax.set_title('Train/Test Split', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/train_test_split.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Data Quality**: All series have minimal missing data and are properly aligned\n",
    "2. **Stationarity**: Price series are non-stationary (as expected), returns are stationary\n",
    "3. **Correlations**: Several highly correlated pairs identified (>0.8)\n",
    "4. **Returns**: Distributions show slight negative skew and excess kurtosis (fat tails)\n",
    "5. **Train/Test**: Clean split with sufficient data in both sets\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Proceed to cointegration testing (Notebook 02)\n",
    "2. Identify statistically significant pairs\n",
    "3. Construct spreads using Kalman Filter\n",
    "4. Generate trading signals and backtest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
